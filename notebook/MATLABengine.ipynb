{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import h5py\n",
    "import imageio\n",
    "import math\n",
    "import matlab.engine\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import skimage\n",
    "import skimage.io\n",
    "import skimage.transform\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm as tqdm\n",
    "from vectormath import Vector2, Vector3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_limits = pickle.load(open('z_limits.dat', 'rb'))\n",
    "z_centers = (z_limits[1:65] + z_limits[0:64]) / 2\n",
    "z_delta = z_limits[32]\n",
    "z_depth = z_limits[1] - z_limits[0]\n",
    "\n",
    "anno = pickle.load(open(\"train.bin\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1005.18590298 -974.153937906\n",
      "-989.669920444 -958.637955369\n",
      "-12.1630205748\n",
      "31.0319650752\n"
     ]
    }
   ],
   "source": [
    "print(z_limits[0], z_limits[1])\n",
    "print(z_centers[0], z_centers[1])\n",
    "print(z_delta)\n",
    "print(z_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'D:/data/Human3.6M/Release-v1.1/'\n",
    "script_paths = [subdir for subdir, _, _ in os.walk(root) if '.git' not in subdir]\n",
    "additional_script_paths = [\n",
    "    # empty\n",
    "]\n",
    "subjects = [\n",
    "    1, 5, 6, 7, 8, # training\n",
    "    9, 11, # validation\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core = matlab.engine.start_matlab()\n",
    "for script_path in script_paths + additional_script_paths:\n",
    "    core.addpath(script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core.workspace['DB'] = core.H36MDataBase.instance()\n",
    "core.workspace['feature_RGB'] = core.H36MRGBVideoFeature()\n",
    "core.workspace['feature_BB'] = core.H36MMyBBMask()\n",
    "core.workspace['feature_BG'] = core.H36MMyBGMask()\n",
    "core.workspace['features'] = [\n",
    "    core.H36MPose2DPositionsFeature(),\n",
    "    core.H36MPose3DPositionsFeature('Monocular', True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_sequence(subject, action, sub_action, camera):\n",
    "    return subject in [1, 5, 6, 7, 8, 9, 11] and\\\n",
    "        1 <= action <= 16 and\\\n",
    "        1 <= sub_action <= 2 and\\\n",
    "        1 <= camera <= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_frame(subject, action, sub_action):\n",
    "    return int(core.getNumFrames(core.workspace['DB'], subject, action, sub_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(subject, action, sub_action, camera):\n",
    "    core.workspace['sequence'] = core.H36MSequence(subject, action, sub_action, camera, -1)\n",
    "    return core.workspace['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_intrinsics(subject, action, sub_action, camera):\n",
    "    if not valid_sequence(subject, action, sub_action, camera):\n",
    "        raise IndexError()\n",
    "    \n",
    "    sequence = get_sequence(subject, action, sub_action, camera)\n",
    "    core.workspace['camera'] = core.getCamera(sequence)\n",
    "    \n",
    "    f, c, k, p = [core.eval('camera.%s' % attrib)[0] for attrib in ['f', 'c', 'k', 'p']]\n",
    "    \n",
    "    return f, c, k, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_RGB(subject, action, sub_action, camera, frame):\n",
    "    if not valid_sequence(subject, action, sub_action, camera):\n",
    "        raise IndexError()\n",
    "    \n",
    "    max_frame = get_max_frame(subject, action, sub_action)\n",
    "    if not (1 <= frame <= max_frame):\n",
    "        raise IndexError()\n",
    "    \n",
    "    sequence = get_sequence(subject, action, sub_action, camera)\n",
    "    core.workspace['metadata'] = core.serializer(core.workspace['feature_RGB'], sequence)\n",
    "    \n",
    "    image = core.getFrame(core.workspace['metadata'], core.double(frame))\n",
    "    image = np.reshape(np.asarray(image._data, dtype=np.float), newshape=(image._size[2], image._size[1], image._size[0])).transpose(2, 1, 0)\n",
    "    \n",
    "    video_name = core.eval('metadata.Reader.VideoName')\n",
    "    \n",
    "    return image, video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_video_name(subject, action, sub_action, camera):\n",
    "    if not valid_sequence(subject, action, sub_action, camera):\n",
    "        raise IndexError()\n",
    "    \n",
    "    sequence = get_sequence(subject, action, sub_action, camera)\n",
    "    core.workspace['metadata'] = core.serializer(core.workspace['feature_RGB'], sequence)\n",
    "    \n",
    "    video_name = core.eval('metadata.Reader.VideoName')\n",
    "    \n",
    "    return video_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pose(subject, action, sub_action, camera, frame):\n",
    "    if not valid_sequence(subject, action, sub_action, camera):\n",
    "        raise IndexError()\n",
    "    \n",
    "    max_frame = get_max_frame(subject, action, sub_action)\n",
    "    if not (1 <= frame <= max_frame):\n",
    "        raise IndexError()\n",
    "    \n",
    "    sequence = get_sequence(subject, action, sub_action, camera)\n",
    "    core.eval('sequence.IdxFrames = %d;' % frame, nargout=0)\n",
    "    \n",
    "    pose = core.H36MComputeFeatures(sequence, core.workspace['features'])\n",
    "    \n",
    "    return np.reshape(np.asarray(pose[0]), newshape=(32, 2)),\\\n",
    "        np.reshape(np.asarray(pose[1]), newshape=(32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_center_scale(subject, action, sub_action, camera, frame):\n",
    "    if not valid_sequence(subject, action, sub_action, camera):\n",
    "        raise IndexError()\n",
    "    \n",
    "    max_frame = get_max_frame(subject, action, sub_action)\n",
    "    if not (1 <= frame <= max_frame):\n",
    "        raise IndexError()\n",
    "    \n",
    "    sequence = get_sequence(subject, action, sub_action, camera)\n",
    "    core.workspace['metadata'] = core.serializer(core.workspace['feature_BB'], sequence)\n",
    "    \n",
    "    mask = core.getFrame(core.workspace['metadata'], core.double(frame))\n",
    "    mask = np.reshape(np.asarray(mask._data, dtype=np.float), newshape=(mask._size[1], mask._size[0])).transpose(1, 0)\n",
    "    \n",
    "    flatten = mask.flatten()\n",
    "    flatten = np.nonzero(flatten)[0]\n",
    "    ul, br = [flatten[where] for where in [0, -1]]\n",
    "    ul = Vector2(ul % mask.shape[1], ul // mask.shape[1])\n",
    "    br = Vector2(br % mask.shape[1], br // mask.shape[1])\n",
    "\n",
    "    center = (ul + br) / 2\n",
    "    height = (br - ul).y\n",
    "    width  = (br - ul).x\n",
    "    scale = max(height, width) / 200\n",
    "    \n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_center_scale_directly(video_name, frame):\n",
    "    \n",
    "    sub = video_name.split('/')[-3].split('\\\\')[0]\n",
    "    act, cam = video_name.split('/')[-1].split('.mp4')[0].split('.')\n",
    "    #act = act.replace(' ', '_')\n",
    "        \n",
    "    data_root = 'D:/data/Human3.6M/downloaded/'\n",
    "    bb_path = os.path.join(data_root, sub, 'MySegmentsMat', 'ground_truth_bb', '%s.%s.mat' % (act, cam))\n",
    "    with h5py.File(bb_path, 'r') as file:\n",
    "        mask = np.asarray(file[file['Masks'][frame][0]]).transpose(1, 0)\n",
    "\n",
    "        flatten = mask.flatten()\n",
    "        flatten = np.nonzero(flatten)[0]\n",
    "        ul, br = [flatten[where] for where in [0, -1]]\n",
    "        ul = Vector2(ul % mask.shape[1], ul // mask.shape[1])\n",
    "        br = Vector2(br % mask.shape[1], br // mask.shape[1])\n",
    "\n",
    "        center = (ul + br) / 2\n",
    "        height = (br - ul).y\n",
    "        width  = (br - ul).x\n",
    "        scale = max(height, width) / 200\n",
    "    \n",
    "    return center, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project(keypoints, f, c, k, p):\n",
    "    X = keypoints.transpose(1, 0) # Already in 3D pose\n",
    "    XX = np.divide(X[0:2, :], X[2, :])\n",
    "    r2 = np.power(XX[0, :], 2) + np.power(XX[1, :], 2)\n",
    "    radial = np.dot(k, np.asarray([r2, np.power(r2, 2), np.power(r2, 3)])) + 1\n",
    "    tan = p[0] * XX[1, :] + p[1] * XX[0, :]\n",
    "    temp = radial + tan\n",
    "    first = XX * np.stack([temp, temp])\n",
    "    second = np.expand_dims(np.asarray([p[1], p[0]]), axis=1) * np.expand_dims(r2, axis=0)\n",
    "    XXX = first + second\n",
    "    XXX = XXX.transpose(1, 0)\n",
    "    proj = f * XXX + c\n",
    "    \n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_image(image, center, scale, rotate, resolution):\n",
    "    center = Vector2(center)  # assign new array\n",
    "    height, width, channel = image.shape\n",
    "    crop_ratio = 200 * scale / resolution\n",
    "    if crop_ratio >= 2:  # if box size is greater than two time of resolution px\n",
    "        # scale down image\n",
    "        height = math.floor(height / crop_ratio)\n",
    "        width = math.floor(width / crop_ratio)\n",
    "\n",
    "        if max([height, width]) < 2:\n",
    "            # Zoomed out so much that the image is now a single pixel or less\n",
    "            raise ValueError(\"Width or height is invalid!\")\n",
    "\n",
    "        image = skimage.transform.resize(image, (height, width), mode='constant')\n",
    "#         image = image.resize(image, (height, width), mode='constant')\n",
    "        center /= crop_ratio\n",
    "        scale /= crop_ratio\n",
    "\n",
    "    ul = (center - 200 * scale / 2).astype(int)\n",
    "    br = (center + 200 * scale / 2).astype(int)  # Vector2\n",
    "\n",
    "    if crop_ratio >= 2:  # force image size 256 x 256\n",
    "        br -= (br - ul - resolution)\n",
    "\n",
    "    pad_length = math.ceil((ul - br).length - (br.x - ul.x) / 2)\n",
    "\n",
    "    if rotate != 0:\n",
    "        ul -= pad_length\n",
    "        br += pad_length\n",
    "\n",
    "    src = [max(0, ul.y), min(height, br.y), max(0, ul.x), min(width, br.x)]\n",
    "    dst = [max(0, -ul.y), min(height, br.y) - ul.y, max(0, -ul.x), min(width, br.x) - ul.x]\n",
    "\n",
    "    new_image = np.zeros([br.y - ul.y, br.x - ul.x, channel], dtype=np.float32)\n",
    "    new_image[dst[0]:dst[1], dst[2]:dst[3], :] = image[src[0]:src[1], src[2]:src[3], :]\n",
    "\n",
    "    if rotate != 0:\n",
    "        new_image = skimage.transform.rotate(new_image, rotate)\n",
    "        new_height, new_width, _ = new_image.shape\n",
    "        new_image = new_image[pad_length:new_height - pad_length, pad_length:new_width - pad_length, :]\n",
    "\n",
    "    if crop_ratio < 2:\n",
    "        new_image = skimage.transform.resize(new_image, (resolution, resolution), mode='constant')\n",
    "#         new_image = Image.resize(new_image, (resolution, resolution), mode='constant')\n",
    "\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subject = 1\n",
    "# action = 2\n",
    "# sub_action = 1\n",
    "# camera = 1\n",
    "# frame = 1\n",
    "\n",
    "# pelvis = [1]\n",
    "# left_leg = [7, 8, 9]\n",
    "# right_leg = [2, 3, 4]\n",
    "# spine = [13, 14, 15, 16]\n",
    "# left_arm = [18, 19, 20]\n",
    "# right_arm = [26, 27, 28]\n",
    "# keypoints = pelvis + left_leg + right_leg + spine + left_arm + right_arm\n",
    "\n",
    "# image, image_name = get_RGB(subject, action, sub_action, camera, frame) # RGB image\n",
    "# center, scale = get_center_scale(subject, action, sub_action, camera, frame) # center, scale\n",
    "# in_image_space, in_camera_space = get_pose(subject, action, sub_action, camera, frame) # part, S\n",
    "# f, c, k, p = get_intrinsics(subject, action, sub_action, camera)\n",
    "\n",
    "# z = in_camera_space[:, -1]\n",
    "# z_center = z[0]\n",
    "# z_index = (z - z_center - z_delta)/(z_depth) + 33\n",
    "# z_index = np.floor(z_index).astype(int) # zidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imageio.imwrite('original.jpg', crop_image(image, center, scale, 0, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dir_center, dir_scale = get_center_scale_directly(image_name, frame)\n",
    "# imageio.imwrite('directly.jpg', crop_image(image, dir_center, dir_scale, 0, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(anno['image'][0])\n",
    "\n",
    "# sub = image_name.split('/')[-3].split('\\\\')[0]\n",
    "# act, cam = image_name.split('/')[-1].split('.mp4')[0].split('.')\n",
    "# act = act.replace(' ', '_')\n",
    "# image_name = '%s_%s.%s_%06d' % (sub, act, cam, frame)\n",
    "# print(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(anno['S'][0])\n",
    "\n",
    "# print(np.reshape([in_camera_space[idx-1] for idx in keypoints], (-1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(anno['part'][0])\n",
    "# print(np.reshape([in_image_space[idx-1] for idx in keypoints], (-1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(anno['scale'][0])\n",
    "# print(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(anno['center'][0])\n",
    "# print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(anno['zind'][0])\n",
    "# print(np.reshape([z_index[idx-1] for idx in keypoints], (-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "subject(6) action(3-2) camera(3):  44%|█████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 136919/311724 [11:05:44<14:09:57,  3.43it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-afca83fdf5ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                             \u001b[1;31m# center, scale = get_center_scale(subject, action, sub_action, camera, frame) # center, scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                             \u001b[0min_image_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_camera_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcamera\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# part, S\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                             \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_camera_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f098a36fde42>\u001b[0m in \u001b[0;36mget_pose\u001b[1;34m(subject, action, sub_action, camera, frame)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmax_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_max_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-d82b0f8bebc0>\u001b[0m in \u001b[0;36mget_max_frame\u001b[1;34m(subject, action, sub_action)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_max_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumFrames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'DB'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "pelvis = [1]\n",
    "left_leg = [7, 8, 9]\n",
    "right_leg = [2, 3, 4]\n",
    "spine = [13, 14, 15, 16]\n",
    "left_arm = [18, 19, 20]\n",
    "right_arm = [26, 27, 28]\n",
    "keypoints = pelvis + left_leg + right_leg + spine + left_arm + right_arm\n",
    "\n",
    "converted = dict()\n",
    "converted ['S'] = list()\n",
    "converted ['part'] = list()\n",
    "converted ['center'] = list()\n",
    "converted ['scale'] = list()\n",
    "converted ['image'] = list()\n",
    "converted ['zind'] = list()\n",
    "\n",
    "total = 0\n",
    "\n",
    "for subject in [1, 5, 6, 7, 8, ]:\n",
    "    for action in range(2, 16 + 1):\n",
    "        for sub_action in [1, 2]:\n",
    "            for camera in [1, 2, 3, 4]:\n",
    "\n",
    "                # Data corrupted.\n",
    "                if subject == 11 and action == 2 and sub_action == 2 and camera == 1:\n",
    "                    continue\n",
    "                \n",
    "                max_frame = get_max_frame(subject, action, sub_action)\n",
    "                total = total + max_frame//5\n",
    "                \n",
    "    \n",
    "with tqdm(total=total) as progress:\n",
    "\n",
    "    for subject in [1, 5, 6, 7, 8, ]:\n",
    "        for action in range(2, 16 + 1):\n",
    "            for sub_action in [1, 2]:\n",
    "                for camera in [1, 2, 3, 4]:\n",
    "\n",
    "                    progress.set_description('subject(%d) action(%d-%d) camera(%d)' % (subject, action, sub_action, camera))\n",
    "\n",
    "                    # Data corrupted.\n",
    "                    if subject == 11 and action == 2 and sub_action == 2 and camera == 1:\n",
    "                        continue\n",
    "\n",
    "                    max_frame = get_max_frame(subject, action, sub_action)\n",
    "\n",
    "                    video_name = get_video_name(subject, action, sub_action, camera)\n",
    "                    sub = video_name.split('/')[-3].split('\\\\')[0]\n",
    "                    act, cam = video_name.split('/')[-1].split('.mp4')[0].split('.')\n",
    "                    \n",
    "                    data_root = 'D:/data/Human3.6M/downloaded/'\n",
    "                    bb_path = os.path.join(data_root, sub, 'MySegmentsMat', 'ground_truth_bb', '%s.%s.mat' % (act, cam))\n",
    "                    \n",
    "                    act = act.replace(' ', '_')\n",
    "                    video_name = '%s_%s.%s' % (sub, act, cam)\n",
    "                    \n",
    "                    with h5py.File(bb_path, 'r') as file:\n",
    "\n",
    "                        for frame in range(1, max_frame+1, 5):\n",
    "                            mask = np.asarray(file[file['Masks'][frame-1][0]]).transpose(1, 0)\n",
    "\n",
    "                            flatten = mask.flatten()\n",
    "                            flatten = np.nonzero(flatten)[0]\n",
    "                            ul, br = [flatten[where] for where in [0, -1]]\n",
    "                            ul = Vector2(ul % mask.shape[1], ul // mask.shape[1])\n",
    "                            br = Vector2(br % mask.shape[1], br // mask.shape[1])\n",
    "\n",
    "                            center = (ul + br) / 2 # center\n",
    "                            height = (br - ul).y\n",
    "                            width  = (br - ul).x\n",
    "                            scale = max(height, width) / 200 # scale\n",
    "                \n",
    "                            # center, scale = get_center_scale(subject, action, sub_action, camera, frame) # center, scale\n",
    "                            in_image_space, in_camera_space = get_pose(subject, action, sub_action, camera, frame) # part, S\n",
    "\n",
    "                            z = in_camera_space[:, -1]\n",
    "                            z_center = z[0]\n",
    "                            z_index = (z - z_center - z_delta)/(z_depth) + 33\n",
    "                            z_index = np.floor(z_index).astype(int) # zidx\n",
    "                            \n",
    "                            for idx in keypoints:\n",
    "                                if not (1 <= z_index[idx-1] <= 64):\n",
    "                                    print(subject, action, sub_action, camera, frame)\n",
    "                                    raise Exception('zind out of range!')\n",
    "\n",
    "                            converted ['S'].append(np.reshape([in_camera_space[idx-1] for idx in keypoints], (-1, 3)))\n",
    "                            converted ['part'].append(np.reshape([in_image_space[idx-1] for idx in keypoints], (-1, 2)))\n",
    "                            converted ['center'].append(center)\n",
    "                            converted ['scale'].append(scale)\n",
    "                            converted ['image'].append('%s_%06d.jpg' % (video_name, frame))\n",
    "                            converted ['zind'].append(np.reshape([z_index[idx-1] for idx in keypoints], (-1)))\n",
    "\n",
    "                            progress.update(1)\n",
    "\n",
    "pickle.dump(converted, open('converted_train.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S6_WalkDog_1.60457274\n",
      "[[  236.1051178   -672.7512207   4491.43652344]\n",
      " [  370.77529907  -691.55163574  4534.43164062]\n",
      " [  386.10348511  -206.30427551  4566.74902344]\n",
      " [  313.61047363   177.60742188  4812.37744141]\n",
      " [  101.43293762  -653.95056152  4448.44091797]\n",
      " [   72.15738678  -270.58151245  4746.63964844]\n",
      " [   38.75497055    26.1042099   5098.54638672]\n",
      " [  290.51269531  -823.11938477  4283.61962891]\n",
      " [  408.41387939  -978.88031006  4112.03076172]\n",
      " [  428.20654297 -1004.84918213  3997.18334961]\n",
      " [  411.77511597 -1111.13061523  4037.91723633]\n",
      " [  454.07437134 -1018.47338867  4248.63378906]\n",
      " [  300.04336548 -1181.57250977  4449.33056641]\n",
      " [  239.09413147 -1318.15759277  4659.44824219]\n",
      " [  316.32446289  -906.9989624   4018.94335938]\n",
      " [  312.87930298  -803.78192139  3736.20654297]\n",
      " [  369.99295044  -776.5869751   3486.16992188]]\n",
      "[33 34 35 43 32 41 52 26 21 17 18 25 32 38 18  9  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "subject = 6\n",
    "action = 15\n",
    "sub_action = 1\n",
    "camera = 4\n",
    "\n",
    "pelvis = [1]\n",
    "left_leg = [7, 8, 9]\n",
    "right_leg = [2, 3, 4]\n",
    "spine = [13, 14, 15, 16]\n",
    "left_arm = [18, 19, 20]\n",
    "right_arm = [26, 27, 28]\n",
    "keypoints = pelvis + left_leg + right_leg + spine + left_arm + right_arm\n",
    "\n",
    "max_frame = get_max_frame(subject, action, sub_action)\n",
    "\n",
    "video_name = get_video_name(subject, action, sub_action, camera)\n",
    "sub = video_name.split('/')[-3].split('\\\\')[0]\n",
    "act, cam = video_name.split('/')[-1].split('.mp4')[0].split('.')\n",
    "\n",
    "data_root = 'D:/data/Human3.6M/downloaded/'\n",
    "bb_path = os.path.join(data_root, sub, 'MySegmentsMat', 'ground_truth_bb', '%s.%s.mat' % (act, cam))\n",
    "\n",
    "act = act.replace(' ', '_')\n",
    "video_name = '%s_%s.%s' % (sub, act, cam)\n",
    "\n",
    "print(video_name)\n",
    "\n",
    "with h5py.File(bb_path, 'r') as file:\n",
    "\n",
    "    frame = 391\n",
    "    \n",
    "    mask = np.asarray(file[file['Masks'][frame-1][0]]).transpose(1, 0)\n",
    "\n",
    "    flatten = mask.flatten()\n",
    "    flatten = np.nonzero(flatten)[0]\n",
    "    ul, br = [flatten[where] for where in [0, -1]]\n",
    "    ul = Vector2(ul % mask.shape[1], ul // mask.shape[1])\n",
    "    br = Vector2(br % mask.shape[1], br // mask.shape[1])\n",
    "\n",
    "    center = (ul + br) / 2 # center\n",
    "    height = (br - ul).y\n",
    "    width  = (br - ul).x\n",
    "    scale = max(height, width) / 200 # scale\n",
    "\n",
    "    # center, scale = get_center_scale(subject, action, sub_action, camera, frame) # center, scale\n",
    "    in_image_space, in_camera_space = get_pose(subject, action, sub_action, camera, frame) # part, S\n",
    "\n",
    "    z = in_camera_space[:, -1]\n",
    "    z_center = z[0]\n",
    "    z_index = (z - z_center - z_delta)/(z_depth) + 33\n",
    "    z_index = np.floor(z_index).astype(int) # zidx\n",
    "            \n",
    "    print(np.reshape([in_camera_space[idx-1] for idx in keypoints], (-1, 3)))\n",
    "    print(np.reshape([z_index[idx-1] for idx in keypoints], (-1)))\n",
    "    \n",
    "    image, image_name = get_RGB(subject, action, sub_action, camera, frame) # RGB image\n",
    "    center, scale = get_center_scale(subject, action, sub_action, camera, frame) # center, scale\n",
    "    in_image_space, in_camera_space = get_pose(subject, action, sub_action, camera, frame) # part, S\n",
    "    f, c, k, p = get_intrinsics(subject, action, sub_action, camera)\n",
    "    \n",
    "    imageio.imwrite('original.jpg', crop_image(image, center, scale, 0, 256))\n",
    "\n",
    "#     converted ['S'].append(np.reshape([in_camera_space[idx-1] for idx in keypoints], (-1, 3)))\n",
    "#     converted ['part'].append(np.reshape([in_image_space[idx-1] for idx in keypoints], (-1, 2)))\n",
    "#     converted ['center'].append(center)\n",
    "#     converted ['scale'].append(scale)\n",
    "#     converted ['image'].append('%s_%06d.jpg' % (video_name, frame))\n",
    "#     converted ['zind'].append(np.reshape([z_index[idx-1] for idx in keypoints], (-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 146.03289332  -91.61660087 -993.02288241]\n",
      "[ 146.05085322  -91.67273382 -993.10358099]\n",
      "[ 37.  30.   1.]\n"
     ]
    }
   ],
   "source": [
    "# for index in range(len(anno['image'])):\n",
    "#     if anno['image'][index] == 'S6_WalkDog_1.60457274_000391.jpg':\n",
    "#         print(index)\n",
    "#         break\n",
    "index = 169790\n",
    "\n",
    "print((anno['S'][index][-1] - anno['S'][index][0] - z_delta))\n",
    "temp = np.reshape([in_camera_space[idx-1] for idx in keypoints], (-1, 3))\n",
    "print(temp[-1] - temp[0] - z_delta)\n",
    "\n",
    "print((anno['S'][index][-1] - anno['S'][index][0] - z_delta) // z_depth + 33)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
