{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite\n",
    "\n",
    "- Install *MATLAB engine API*\n",
    "- Update *VideoUtils* under *${Human3.6M code}/external_utils/VideoUtils_v1_2/*\n",
    "- Change all MATLAB classes to inherit *handle* class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import imageio\n",
    "import matlab.engine\n",
    "import numpy as np\n",
    "from os import path, walk\n",
    "from os.path import abspath, curdir, exists, basename\n",
    "import cv2\n",
    "import copy\n",
    "import math\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class H36M(object):\n",
    "    '''Parser for Human3.6M.\n",
    "    \n",
    "    With MATLAB engine, this class runs Human3.6M MATLAB scripts and gets the data.\n",
    "    \n",
    "    Attrib:\n",
    "        root: The path to Human3.6M MATLAB scripts directory.\n",
    "        script_paths: The sub-directory paths excluding git directory.\n",
    "        additional_script_paths: The paths to the customized MATLAB scripts.\n",
    "        \n",
    "        _core: The MATLAB engine object.\n",
    "    '''\n",
    "    root = abspath(path.join('D:', 'data', 'Human3.6M', 'Release-v1.1'))\n",
    "    script_paths = [subdir for subdir, _, _ in walk(root) if '.git' not in subdir]\n",
    "    additional_script_paths = [\n",
    "    ]\n",
    "    subjects = [1, 5, 6, 7, 8, 9, 11]\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''Initialize the MATLAB engine and preload common instances.\n",
    "        '''\n",
    "        \n",
    "        # Initialize the MATLAB engine object.\n",
    "        self._core = matlab.engine.start_matlab()\n",
    "        for script_path in H36M.script_paths + H36M.additional_script_paths:\n",
    "            self._core.addpath(script_path)\n",
    "\n",
    "        # Preload H36MDataBase and necessary metadata of annotations.\n",
    "        self._core.workspace['DB'] = self._core.H36MDataBase.instance()\n",
    "        self._core.workspace['feature_RGB'] = self._core.H36MRGBVideoFeature()\n",
    "        self._core.workspace['feature_BB'] = self._core.H36MMyBBMask()\n",
    "        self._core.workspace['feature_BG'] = self._core.H36MMyBGMask()\n",
    "        self._core.workspace['features'] = [\n",
    "            # self._core.H36MPose3DPositionsFeature(),\n",
    "            # self._core.H36MPose2DPositionsFeature(), # Use Camera.project()\n",
    "            self._core.H36MPose3DPositionsFeature('Monocular', True), # Use TransformJointsPosition()\n",
    "        ]\n",
    "    \n",
    "    @property\n",
    "    def workspace(self):\n",
    "        return self._core.workspace\n",
    "    \n",
    "    def __call__(self, inst, exe_only=False):\n",
    "        if exe_only:\n",
    "            return self._core.eval(inst, nargout=0)\n",
    "        return self._core.eval(inst)\n",
    "    \n",
    "    def RGB_image(self, subject, action, sub_action, camera, frame):\n",
    "        '''Get a single RGB image.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "            frame: The frame number.\n",
    "        \n",
    "        Return:\n",
    "            RGB image as numpy array in the height-width-channel shape.\n",
    "        '''\n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "        \n",
    "        max_frame = self.max_frame(subject, action, sub_action)\n",
    "        if not (1 <= frame <= max_frame):\n",
    "            raise IndexError()\n",
    "        \n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        RGB = self._RGB_handle(subject, action, sub_action, camera)\n",
    "        image = self._core.getFrame(RGB, self._core.double(frame))\n",
    "        image = np.reshape(np.asarray(image._data, dtype=np.float), newshape=(image._size[2], image._size[1], image._size[0])).transpose(2, 1, 0)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def camera_intrinsics(self, subject, action, sub_action, camera, raw_data=False):\n",
    "        \n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "            \n",
    "        seq = self._sequence(subject, action, sub_action, camera)\n",
    "        self.workspace['tmp'] = self._core.getCamera(seq)\n",
    "\n",
    "        f, c, k, p = [self._core.eval('tmp.' + prop)[0] for prop in ['f', 'c', 'k', 'p']]\n",
    "\n",
    "        self._core.eval('clear tmp;', nargout=0)\n",
    "        \n",
    "        if raw_data:\n",
    "            return f, c, k, p\n",
    "\n",
    "        distort = np.asarray([k[0], k[1], p[0], p[1], k[2]])\n",
    "        cam_mat = np.mat('%f 0 %f; 0 %f %f; 0 0 1' % (f[0], c[0], f[1], c[1]))\n",
    "        \n",
    "        return distort, cam_mat\n",
    "    \n",
    "    def undistorted_RGB_image(self, subject, action, sub_action, camera, frame):\n",
    "        \n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "            \n",
    "        distort, cam_mat = self.camera_intrinsics(subject, action, sub_action, camera)\n",
    "        image = self.RGB_image(subject, action, sub_action, camera, frame)\n",
    "        \n",
    "        h,  w = image.shape[:2]\n",
    "        newcameramtx, roi = cv2.getOptimalNewCameraMatrix(cam_mat,distort,(w,h),1,(w,h))\n",
    "\n",
    "        # undistort\n",
    "        dst = cv2.undistort(image, cam_mat, distort, None, newcameramtx)\n",
    "\n",
    "        # crop the image\n",
    "        x,y,w,h = roi\n",
    "        dst = dst[y:y+h, x:x+w]\n",
    "        \n",
    "        return dst\n",
    "    \n",
    "    def BB_mask(self, subject, action, sub_action, camera, frame):\n",
    "        '''Get a single bounding-box mask.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "            frame: The frame number.\n",
    "        \n",
    "        Return:\n",
    "            The bounding-box mask as numpy array in the height-width shape.\n",
    "        '''\n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "        \n",
    "        max_frame = self.max_frame(subject, action, sub_action)\n",
    "        if not (1 <= frame <= max_frame):\n",
    "            raise IndexError()\n",
    "        \n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        BB = self._BB_handle(subject, action, sub_action, camera)\n",
    "        mask = self._core.getFrame(BB, self._core.double(frame))\n",
    "        mask = np.reshape(np.asarray(mask._data, dtype=np.float), newshape=(mask._size[1], mask._size[0])).transpose(1, 0)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def BG_mask(self, subject, action, sub_action, camera, frame):\n",
    "        '''Get a single background mask.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "            frame: The frame number.\n",
    "        \n",
    "        Return:\n",
    "            The background mask as numpy array in the height-width shape.\n",
    "        '''\n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "        \n",
    "        max_frame = self.max_frame(subject, action, sub_action)\n",
    "        if not (1 <= frame <= max_frame):\n",
    "            raise IndexError()\n",
    "        \n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        BG = self._BG_handle(subject, action, sub_action, camera)\n",
    "        mask = self._core.getFrame(BG, self._core.double(frame))\n",
    "        mask = np.reshape(np.asarray(mask._data, dtype=np.float), newshape=(mask._size[1], mask._size[0])).transpose(1, 0)\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def ToF(self, subject, action, sub_action, frame):\n",
    "        '''Get a single time-of-flight image.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            frame: The frame number.\n",
    "        \n",
    "        Return:\n",
    "            The background mask as numpy array in the height-width shape.\n",
    "        '''\n",
    "        if not self.valid_sequence(subject, action, sub_action, camera = 2):\n",
    "            raise IndexError()\n",
    "        \n",
    "        max_frame = self.max_frame(subject, action, sub_action)\n",
    "        if not (1 <= frame <= max_frame):\n",
    "            raise IndexError()\n",
    "        \n",
    "        ToF = self._ToF_handle(subject, action, sub_action)\n",
    "        ToF = self._core.getFrame(ToF, self._core.double(frame))\n",
    "        ToF = np.reshape(np.asarray(ToF._data, dtype=np.float), newshape=(ToF._size[1], ToF._size[0])).transpose(1, 0)\n",
    "\n",
    "        return ToF\n",
    "    \n",
    "    def RGB_video_name(self, subject, action, sub_action, camera):\n",
    "        '''Get the RGB video file name.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "        \n",
    "        Return:\n",
    "            The file name of RGB video.\n",
    "        '''\n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "        \n",
    "        RGB = self._RGB_handle(subject, action, sub_action, camera)\n",
    "        var_name = 'RGB_%02d_%02d_%d_%d' % (subject, action, sub_action, camera)\n",
    "        \n",
    "        return self._core.eval('%s.Reader.VideoName;' % var_name)\n",
    "    \n",
    "    def pose_3D(self, subject, action, sub_action, camera, frame):\n",
    "        '''Get the keypoint positions.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "            frame: The frame number.\n",
    "        \n",
    "        Return:\n",
    "            32 Keypoint 3D positions.\n",
    "        '''\n",
    "        if not self.valid_sequence(subject, action, sub_action, camera):\n",
    "            raise IndexError()\n",
    "        \n",
    "        max_frame = self.max_frame(subject, action, sub_action)\n",
    "        if not (1 <= frame <= max_frame):\n",
    "            raise IndexError()\n",
    "        \n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        var_name = 'sequence_%02d_%02d_%d_%d' % (subject, action, sub_action, camera)\n",
    "        self._core.eval('%s.IdxFrames = %d;' % (var_name, frame), nargout = 0)\n",
    "    \n",
    "        pose_3D = self._core.H36MComputeFeatures(sequence, self._core.workspace['features'])[0]\n",
    "        return np.reshape(np.asarray(pose_3D), newshape=(32, 3))\n",
    "    \n",
    "    def valid_sequence(self, subject, action, sub_action, camera):\n",
    "        '''Check if the sequence is valid.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "            frame: The frame number.\n",
    "        \n",
    "        Return:\n",
    "            True if the valid sequence.\n",
    "            Otherwise False.\n",
    "        '''\n",
    "        return subject in H36M.subjects and\\\n",
    "            1 <= action <= 16 and\\\n",
    "            1 <= sub_action <= 2 and\\\n",
    "            1 <= camera <= 4\n",
    "    \n",
    "    def max_frame(self, subject, action, sub_action):\n",
    "        '''Get the maximum frame.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "        \n",
    "        Return:\n",
    "            The maximum frame.\n",
    "        '''\n",
    "        return self._core.getNumFrames(self._core.workspace['DB'], subject, action, sub_action)\n",
    "    \n",
    "    def clear_workspace(self):\n",
    "        '''Clear the MATLAB workspace.\n",
    "        '''\n",
    "        self._core.eval('clear; close all;', nargout = 0)\n",
    "    \n",
    "    @lru_cache(maxsize = 1024)\n",
    "    def _sequence(self, subject, action, sub_action, camera):\n",
    "        '''Covert to sequence_object.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "        \n",
    "        Return:\n",
    "            H36MSequence object.\n",
    "        '''\n",
    "        var_name = 'sequence_%02d_%02d_%d_%d' % (subject, action, sub_action, camera)\n",
    "        self._core.workspace[var_name] = self._core.H36MSequence(subject, action, sub_action, camera, -1)\n",
    "        \n",
    "        return self._core.workspace[var_name]\n",
    "    \n",
    "    @lru_cache(maxsize = 1024)\n",
    "    def _RGB_handle(self, subject, action, sub_action, camera):\n",
    "        '''Get RGB video handle object.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "        \n",
    "        Return:\n",
    "            The MATLAB VideoPlayer.\n",
    "        '''\n",
    "        var_name = 'RGB_%02d_%02d_%d_%d' % (subject, action, sub_action, camera)\n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        self._core.workspace[var_name] = self._core.serializer(self._core.workspace['feature_RGB'], sequence)\n",
    "        \n",
    "        return self._core.workspace[var_name]\n",
    "    \n",
    "    @lru_cache(maxsize = 1024)\n",
    "    def _BB_handle(self, subject, action, sub_action, camera):\n",
    "        '''Get the bounding-box handle object.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "        \n",
    "        Return:\n",
    "            The MATLAB VideoPlayer.\n",
    "        '''\n",
    "        var_name = 'BB_%02d_%02d_%d_%d' % (subject, action, sub_action, camera)\n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        self._core.workspace[var_name] = self._core.serializer(self._core.workspace['feature_BB'], sequence)\n",
    "        \n",
    "        return self._core.workspace[var_name]\n",
    "    \n",
    "    @lru_cache(maxsize = 1024)\n",
    "    def _BG_handle(self, subject, action, sub_action, camera):\n",
    "        '''Get the background handle object.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "            camera: The camera number in the range between 1 and 4.\n",
    "        \n",
    "        Return:\n",
    "            The MATLAB VideoPlayer.\n",
    "        '''\n",
    "        var_name = 'BG_%02d_%02d_%d_%d' % (subject, action, sub_action, camera)\n",
    "        sequence = self._sequence(subject, action, sub_action, camera)\n",
    "        self._core.workspace[var_name] = self._core.serializer(self._core.workspace['feature_BG'], sequence)\n",
    "        \n",
    "        return self._core.workspace[var_name]\n",
    "    \n",
    "    @lru_cache(maxsize = 256)\n",
    "    def _ToF_handle(self, subject, action, sub_action):\n",
    "        '''Get the time-of-flight handle object.\n",
    "        \n",
    "        Params:\n",
    "            subject: The subject number in 1, 5, 6, 7, 8, 9 and 11.\n",
    "            action: The action number in the range between 1 and 16.\n",
    "            sub_action: The sub-action number 1 or 2.\n",
    "        \n",
    "        Return:\n",
    "            The ToF cdf file wrapper.\n",
    "        '''\n",
    "        var_name = 'ToF_%02d_%02d_%d' % (subject, action, sub_action)\n",
    "        self._core.workspace[var_name] = self._core.H36MTOFDataAccess(subject, action, sub_action)\n",
    "        \n",
    "        return self._core.workspace[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = H36M()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subject = 1\n",
    "action = 2\n",
    "sub_action = 1\n",
    "camera = 4\n",
    "frame = query.max_frame(subject, action, sub_action)\n",
    "frame = 751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for subject in [1]:\n",
    "    for action in range(2, 16+1):\n",
    "        for sub_action in [1, 2]:\n",
    "            for camera in [1, 2, 3, 4]:\n",
    "                name = query.RGB_video_name(subject, action, sub_action, camera)\n",
    "                # name = basename(name)[:-4].replace(' ', '_')\n",
    "                \n",
    "                if (int(query.max_frame(subject, action, sub_action)) - 1) % 5 == 0:\n",
    "                    cnt = cnt + 1\n",
    "                \n",
    "                continue\n",
    "                \n",
    "                for frame in tqdm(range(1, int(query.max_frame(subject, action, sub_action)) + 1, 5)):\n",
    "                    '''\n",
    "                    file = 'S%d_%s_%05d' % (subject, name, frame)\n",
    "                    print(file)\n",
    "                    break\n",
    "                    '''\n",
    "                    \n",
    "                    name = 'tmp/RGB_%02d_%02d_%d_%d_%04d.png' % (subject, action, sub_action, camera, frame)\n",
    "                    if exists(name):\n",
    "                        continue\n",
    "                    image = query.RGB_image(subject, action, sub_action, camera, frame)\n",
    "                    imageio.imwrite(name, np.asarray(255 * image, dtype=np.uint8))\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = query.RGB_video_name(subject, action, sub_action, camera)\n",
    "print('Video at: %s' % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dst = query.undistorted_RGB_image(subject, action, sub_action, camera, frame)\n",
    "imageio.imwrite('tmp/dst.png', dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = query.RGB_image(subject, action, sub_action, camera, frame)\n",
    "imageio.imwrite('tmp/original.png', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose = query.pose_3D(subject, action, sub_action, camera, frame)\n",
    "print(pose.shape)\n",
    "pelvis = [1]\n",
    "left_leg = [7, 8, 9]\n",
    "right_leg = [2, 3, 4]\n",
    "spine = [13, 14, 15, 16]\n",
    "left_arm = [18, 19, 20]\n",
    "right_arm = [26, 27, 28]\n",
    "for idx in pelvis + left_leg + right_leg + spine + left_arm + right_arm:\n",
    "    print('[%f, %f, %f],' % (pose[idx - 1][0], pose[idx - 1][1], pose[idx - 1][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, c, k, p = query.camera_intrinsics(subject, action, sub_action, camera, raw_data=True)\n",
    "print(f)\n",
    "print(c)\n",
    "print(k)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54138969\n",
      "55011271\n",
      "58860488\n",
      "60457274\n"
     ]
    }
   ],
   "source": [
    "for camera in range(1, 4 + 1):\n",
    "    sequence = query._sequence(subject, action, sub_action, camera)\n",
    "    query.workspace['cam'] = query._core.getCamera(sequence)\n",
    "    f, c, k, p = query.camera_intrinsics(subject, action, sub_action, camera, raw_data=True)\n",
    "    cam = query._core.eval('cam.Name')\n",
    "    print(cam)\n",
    "    np.savetxt('calibration/%s_f.txt' % cam, f)\n",
    "    np.savetxt('calibration/%s_c.txt' % cam, c)\n",
    "    np.savetxt('calibration/%s_k.txt' % cam, k)\n",
    "    np.savetxt('calibration/%s_p.txt' % cam, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def project(keypoints, f, c, k, p):\n",
    "    X = keypoints.transpose(1, 0) # Already in 3D pose\n",
    "    XX = np.divide(X[0:2, :], X[2, :])\n",
    "    r2 = np.power(XX[0, :], 2) + np.power(XX[1, :], 2)\n",
    "    radial = np.dot(k, np.asarray([r2, np.power(r2, 2), np.power(r2, 3)])) + 1\n",
    "    tan = p[0] * XX[1, :] + p[1] * XX[0, :]\n",
    "    temp = radial + tan\n",
    "    first = XX * np.stack([temp, temp])\n",
    "    second = np.expand_dims(np.asarray([p[1], p[0]]), axis=1) * np.expand_dims(r2, axis=0)\n",
    "    XXX = first + second\n",
    "    XXX = XXX.transpose(1, 0)\n",
    "    proj = f * XXX + c\n",
    "    \n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capture_box_length = 2400\n",
    "capture_box_half_length = capture_box_length / 2\n",
    "\n",
    "root = query.pose_3D(subject, action, sub_action, camera, frame)[0, :]\n",
    "\n",
    "x_axis = np.asarray([1, 0, -root[0]/root[2]])\n",
    "y_axis = np.cross(root, x_axis)\n",
    "x_axis = x_axis / np.linalg.norm(x_axis)\n",
    "y_axis = y_axis / np.linalg.norm(y_axis)\n",
    "z_axis = root / np.linalg.norm(root)\n",
    "\n",
    "broadcasting = np.asarray([\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, -1],\n",
    "    [1, -1, 1],\n",
    "    [1, -1, -1],\n",
    "    [-1, 1, 1],\n",
    "    [-1, 1, -1],\n",
    "    [-1, -1, 1],\n",
    "    [-1, -1, -1],\n",
    "])\n",
    "capture_box = np.asarray([\n",
    "    root\n",
    "    + capture_box_half_length * multiplier[0] * x_axis\n",
    "    + capture_box_half_length * multiplier[1] * y_axis\n",
    "    + capture_box_half_length * multiplier[2] * z_axis for multiplier in broadcasting])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = project(np.append(capture_box, pose, axis=0), f, c, k, p)\n",
    "\n",
    "image = query.RGB_image(subject, action, sub_action, camera, frame)\n",
    "for point in cb:\n",
    "    x, y = point.astype(int)\n",
    "    for tx in range(-5, 5):\n",
    "        for ty in range(-5, 5):\n",
    "            xx = x + tx\n",
    "            yy = y + ty\n",
    "            if not 0 <= xx < image.shape[1] or not 0 <= yy < image.shape[0]:\n",
    "                continue\n",
    "            image[yy, xx, :] = [1, 0, 0]\n",
    "imageio.imwrite('projected.png', image)\n",
    "\n",
    "distort, cam_mat = query.camera_intrinsics(subject, action, sub_action, camera)\n",
    "\n",
    "h,  w = image.shape[:2]\n",
    "newcameramtx, roi = cv2.getOptimalNewCameraMatrix(cam_mat,distort,(w,h),1,(w,h))\n",
    "\n",
    "# undistort\n",
    "dst = cv2.undistort(image, cam_mat, distort, None, newcameramtx)\n",
    "\n",
    "# crop the image\n",
    "x,y,w,h = roi\n",
    "dst = dst[y:y+h, x:x+w]\n",
    "imageio.imwrite('projected_distorted.png', dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pose = query.pose_3D(subject, action, sub_action, camera, frame)\n",
    "for idx, keypoint in enumerate(pose):\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    pose[idx] = keypoint - pose[0]\n",
    "pose[0, :] = [0, 0, 0]\n",
    "pose_backup = copy.deepcopy(pose)\n",
    "for idx, keypoint in enumerate(pose):\n",
    "    pose[idx] = [x_axis[0] * keypoint[0] + y_axis[0] * keypoint[1] + z_axis[0] * keypoint[2],\n",
    "                 x_axis[1] * keypoint[0] + y_axis[1] * keypoint[1] + z_axis[1] * keypoint[2],\n",
    "                 x_axis[2] * keypoint[0] + y_axis[2] * keypoint[1] + z_axis[2] * keypoint[2],]\n",
    "\n",
    "x_axis = 0\n",
    "y_axis = 2\n",
    "z_axis = 1\n",
    "\n",
    "pose[:, z_axis] = -pose[:, z_axis]\n",
    "pose_backup[:, z_axis] = -pose_backup[:, z_axis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query._core.eval('close all;', nargout=0)\n",
    "query._core.workspace['fig'] = query('figure(1);')\n",
    "query._core.eval('view(180, 90);', nargout=0)\n",
    "# query('axis([-2000, 2000, -2000, 2000, 0, 2000])', exe_only=True)\n",
    "# query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "spine = np.array([pose[x] for x in [0, 11, 12, 13, 14, 15]])\n",
    "query._core.plot3(\n",
    "    matlab.double(spine[:, x_axis].tolist()),\n",
    "    matlab.double(spine[:, y_axis].tolist()),\n",
    "    matlab.double(spine[:, z_axis].tolist()),\n",
    "    'Color', 'red', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "right_leg = np.array([pose[x] for x in [0, 1, 2, 3, 4]])\n",
    "query._core.plot3(\n",
    "    matlab.double(right_leg[:, x_axis].tolist()),\n",
    "    matlab.double(right_leg[:, y_axis].tolist()),\n",
    "    matlab.double(right_leg[:, z_axis].tolist()),\n",
    "    'Color', 'green', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "left_leg = np.array([pose[x] for x in [0, 6, 7, 8, 9]])\n",
    "query._core.plot3(\n",
    "    matlab.double(left_leg[:, x_axis].tolist()),\n",
    "    matlab.double(left_leg[:, y_axis].tolist()),\n",
    "    matlab.double(left_leg[:, z_axis].tolist()),\n",
    "    'Color', 'blue', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "left_arm = np.array([pose[x] for x in [12, 16, 17, 18, 19, 21]])\n",
    "query._core.plot3(\n",
    "    matlab.double(left_arm[:, x_axis].tolist()),\n",
    "    matlab.double(left_arm[:, y_axis].tolist()),\n",
    "    matlab.double(left_arm[:, z_axis].tolist()),\n",
    "    'Color', 'blue', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "right_arm = np.array([pose[x] for x in [12, 24, 25, 26, 27, 28, 30]])\n",
    "query._core.plot3(\n",
    "    matlab.double(right_arm[:, x_axis].tolist()),\n",
    "    matlab.double(right_arm[:, y_axis].tolist()),\n",
    "    matlab.double(right_arm[:, z_axis].tolist()),\n",
    "    'Color', 'green', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "query._core.eval('grid on;', nargout=0)\n",
    "query._core.eval('axis([-1200, 1200, -1200, 1200, -1200, 1200]);', nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query._core.workspace['fig2'] = query('figure(2);')\n",
    "query._core.eval('view(180, 90);', nargout=0)\n",
    "# query('axis([-2000, 2000, -2000, 2000, 0, 2000])', exe_only=True)\n",
    "# query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "spine = np.array([pose_backup[x] for x in [0, 11, 12, 13, 14, 15]])\n",
    "query._core.plot3(\n",
    "    matlab.double(spine[:, x_axis].tolist()),\n",
    "    matlab.double(spine[:, y_axis].tolist()),\n",
    "    matlab.double(spine[:, z_axis].tolist()),\n",
    "    'Color', 'red', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "right_leg = np.array([pose_backup[x] for x in [0, 1, 2, 3, 4]])\n",
    "query._core.plot3(\n",
    "    matlab.double(right_leg[:, x_axis].tolist()),\n",
    "    matlab.double(right_leg[:, y_axis].tolist()),\n",
    "    matlab.double(right_leg[:, z_axis].tolist()),\n",
    "    'Color', 'green', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "left_leg = np.array([pose_backup[x] for x in [0, 6, 7, 8, 9]])\n",
    "query._core.plot3(\n",
    "    matlab.double(left_leg[:, x_axis].tolist()),\n",
    "    matlab.double(left_leg[:, y_axis].tolist()),\n",
    "    matlab.double(left_leg[:, z_axis].tolist()),\n",
    "    'Color', 'blue', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "left_arm = np.array([pose_backup[x] for x in [12, 16, 17, 18, 19, 21]])\n",
    "query._core.plot3(\n",
    "    matlab.double(left_arm[:, x_axis].tolist()),\n",
    "    matlab.double(left_arm[:, y_axis].tolist()),\n",
    "    matlab.double(left_arm[:, z_axis].tolist()),\n",
    "    'Color', 'blue', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "right_arm = np.array([pose_backup[x] for x in [12, 24, 25, 26, 27, 28, 30]])\n",
    "query._core.plot3(\n",
    "    matlab.double(right_arm[:, x_axis].tolist()),\n",
    "    matlab.double(right_arm[:, y_axis].tolist()),\n",
    "    matlab.double(right_arm[:, z_axis].tolist()),\n",
    "    'Color', 'green', nargout=0)\n",
    "query._core.eval('hold on;', nargout=0)\n",
    "\n",
    "query._core.eval('grid on;', nargout=0)\n",
    "query._core.eval('axis([-1200, 1200, -1200, 1200, -1200, 1200]);', nargout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Vector3[] positions = new [] { ')\n",
    "for part in [right_arm]: #spine, right_arm, left_arm, right_leg, right_leg]:\n",
    "    for point in part * 0.03:\n",
    "        print('\\tnew Vector3(%.4ff, %.4ff, %.4ff), ' % (point[0], point[1], point[2]))\n",
    "print('};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = query.BB_mask(subject, action, sub_action, camera, frame)\n",
    "imageio.imwrite('tmp/bounding_box.png', bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ul = [-1, -1]\n",
    "for y in range(0, bb.shape[0]):\n",
    "    for x in range(0, bb.shape[1]):\n",
    "        if bb[y, x] != 0:\n",
    "            ul = [x, y]\n",
    "            break\n",
    "    if ul != [-1, -1]:\n",
    "        break\n",
    "        \n",
    "br = [-1, -1]\n",
    "for y in range(bb.shape[0] - 1, -1, -1):\n",
    "    for x in range(bb.shape[1] - 1, -1, -1):\n",
    "        if bb[y, x] != 0:\n",
    "            br = [x, y]\n",
    "            break\n",
    "    if br != [-1, -1]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ul, br)\n",
    "print((np.asarray(ul) + np.asarray(br))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for part_idx in tqdm(range(len(annotation['part'][image_idx]))):\n",
    "# x, y = annotation['part'][image_idx][part_idx].astype(int)\n",
    "image = query.RGB_image(subject, action, sub_action, camera, frame)\n",
    "\n",
    "for y in range(0, bb.shape[0]):\n",
    "    for x in range(0, bb.shape[1]):\n",
    "        if bb[y, x] != 0:\n",
    "            image[y, x] = image[y, x] * 0.5 + [0, 0.5, 0]\n",
    "\n",
    "x, y = ul\n",
    "for tx in range(-5, 5):\n",
    "    for ty in range(-5, 5):\n",
    "        xx = x + tx\n",
    "        yy = y + ty\n",
    "        if not 0 <= xx < image.shape[1] or not 0 <= yy < image.shape[0]:\n",
    "            continue\n",
    "        image[yy, xx, :] = [1, 0, 0]\n",
    "        \n",
    "x, y = br\n",
    "for tx in range(-5, 5):\n",
    "    for ty in range(-5, 5):\n",
    "        xx = x + tx\n",
    "        yy = y + ty\n",
    "        if not 0 <= xx < image.shape[1] or not 0 <= yy < image.shape[0]:\n",
    "            continue\n",
    "        image[yy, xx, :] = [1, 0, 0]\n",
    "\n",
    "x, y = [ 387, 367]# [ 499, 462]# [ 466, 465]\n",
    "for tx in range(-5, 5):\n",
    "    for ty in range(-5, 5):\n",
    "        xx = x + tx\n",
    "        yy = y + ty\n",
    "        if not 0 <= xx < image.shape[1] or not 0 <= yy < image.shape[0]:\n",
    "            continue\n",
    "        image[yy, xx, :] = [0, 0, 1]\n",
    "        \n",
    "imageio.imwrite('test.png', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = query.BG_mask(subject, action, sub_action, camera, frame)\n",
    "imageio.imwrite('bg.png', bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tof = query.ToF(subject, action, sub_action, frame)\n",
    "tof /= np.max(tof)\n",
    "imageio.imwrite('tof.png', tof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine.workspace['Features'] = [\n",
    "    engine.H36MPose3DPositionsFeature(),\n",
    "    engine.H36MPose2DPositionsFeature(),\n",
    "    engine.H36MPose3DPositionsFeature('Monocular', True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine.workspace['Sequence'] = engine.H36MSequence(query.subject, query.action, query.sub_action, query.camera, query.frame)\n",
    "engine.workspace['F'] = engine.H36MComputeFeatures(engine.workspace['Sequence'], engine.workspace['Features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine.workspace['c'] = engine.getCamera(engine.workspace['Sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine.workspace['projected'] = engine.project(engine.workspace['c'], engine.workspace['F'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "engine.workspace['camera0'] = engine.H36MCamera(engine.workspace['DB'], 0, 1)\n",
    "engine.workspace['projected'] = engine.project(engine.workspace['camera0'], engine.workspace['F'][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
